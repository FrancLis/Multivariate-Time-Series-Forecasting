{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_Training-Selection-Evaluation_RNN .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPWDprH1rgzGooKELJ2WhL9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancLis/Multivariate-Time-Series-Forecasting/blob/main/Model_Training_Selection_Evaluation_RNN_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fast_ml\n",
        "!pip install talos\n",
        "!pip install kats\n",
        "!pip install scipy\n",
        "!pip install joblib"
      ],
      "metadata": {
        "id": "j3_7phLIJtws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value = 0\n",
        "\n",
        "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as python_random\n",
        "\n",
        "# The below is necessary for starting Numpy generated random numbers\n",
        "# in a well-defined initial state.\n",
        "np.random.seed(123)\n",
        "\n",
        "# The below is necessary for starting core Python generated random numbers\n",
        "# in a well-defined state.\n",
        "python_random.seed(123)\n",
        "\n",
        "# The below set_seed() will make random number generation\n",
        "# in the TensorFlow backend have a well-defined initial state.\n",
        "# For further details, see:\n",
        "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
        "tf.random.set_seed(1234)\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import talos as ta\n",
        "from joblib import dump, load\n",
        "from matplotlib import pyplot as plt\n",
        "from fast_ml.model_development import train_valid_test_split\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import MinMaxScaler, PowerTransformer, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, max_error, mean_absolute_error\n",
        "from tensorflow.keras import Sequential, layers, callbacks\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, GRU, Bidirectional, SimpleRNN, Conv1D, MaxPooling1D, Flatten\n"
      ],
      "metadata": {
        "id": "Xhf0TVeCAITf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Acquisition..."
      ],
      "metadata": {
        "id": "PED-JLJ-TiV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Visualization..."
      ],
      "metadata": {
        "id": "ONXQADopI4Vc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data prepocessing..."
      ],
      "metadata": {
        "id": "z8LPDz-6sU1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Choice and Learning"
      ],
      "metadata": {
        "id": "1C5D4-G1XC32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and read preprocessed data"
      ],
      "metadata": {
        "id": "sBvYW-71LYh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Preprocessed_data_PG.npy', 'rb') as f:\n",
        "    X_train = np.load(f)\n",
        "    y_train = np.load(f)\n",
        "    X_valid = np.load(f)\n",
        "    y_valid = np.load(f)\n",
        "    X_test = np.load(f)\n",
        "    y_test = np.load(f)\n",
        "\n",
        "print('All shapes are: (batch, time, features)')\n",
        "print('X_train.shape:', X_train.shape, 'y_train.shape:', y_train.shape)\n",
        "print('X_valid.shape:', X_valid.shape, 'y_valid.shape:', y_valid.shape)\n",
        "print('X_test.shape:', X_test.shape, 'y_test.shape:', y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkcaZZKFd5QA",
        "outputId": "8a81d541-8d9e-4c6d-babc-61ffa6cf00a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All shapes are: (batch, time, features)\n",
            "X_train.shape: (10496, 20, 5) y_train.shape: (10496, 5)\n",
            "X_valid.shape: (1294, 20, 5) y_valid.shape: (1294, 5)\n",
            "X_test.shape: (1295, 20, 5) y_test.shape: (1295, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load scaler"
      ],
      "metadata": {
        "id": "Es-3SjBjTGax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = load('MinMaxScaler_PG.joblib')"
      ],
      "metadata": {
        "id": "LDFUkU63TGsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It was verified through these lines of code that the two scales were equal"
      ],
      "metadata": {
        "id": "ByOexPRLbboE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Those attributes are specific of MinMaxScaler. For other scalers they might change\n",
        "# if (imported_scaler.scale_ == scaler.scale_).all() and (imported_scaler.data_max_ == scaler.data_max_).all() \\\n",
        "        # and (imported_scale.data_min_ == scaler.data_min_).all() and (imported_scale.data_range_ == scaler.data_range_).all():\n",
        "   # print(\"Scalers are same\")"
      ],
      "metadata": {
        "id": "eY-by2p2bm0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is necessary to import the shoe ladder then subsequently the new observations that the algorithm will predict in the future which will then be reused for the next prison prediction"
      ],
      "metadata": {
        "id": "sY4y8A4zc_Ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from talos.utils import hidden_layers"
      ],
      "metadata": {
        "id": "m6k_ZsXzICT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hidden layers function has been imported from the Talos library but has been modified according to the needs of the project"
      ],
      "metadata": {
        "id": "Yx941nYNdFkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def network_shape_customized(params, last_neuron, network_type):\n",
        "\n",
        "    '''Provides the ability to include network shape in experiments. If params\n",
        "    dictionary for the round contains float value for params['shapes'] then\n",
        "    a linear contraction towards the last_neuron value. The higher the value,\n",
        "    the fewer layers it takes to reach lesser than last_neuron.\n",
        "    Supports three inbuilt shapes 'brick', 'funnel', and 'triangle'.\n",
        "    params : dict\n",
        "         Scan() params for a single roundself.\n",
        "    last_neuron : int\n",
        "         Number of neurons on the output layer in the Keras model.\n",
        "    '''\n",
        "    import numpy as np\n",
        "    from talos.utils.exceptions import TalosParamsError\n",
        "\n",
        "    layers = params['hidden_layers']\n",
        "    shape = params['shapes']\n",
        "    # network_type == 0 --> SimpleRNN\n",
        "    # network_type == 1 --> GRU\n",
        "    # network_type == 2 --> LSTM\n",
        "    # network_type == 3 --> CONV1D\n",
        "    if network_type == 3:\n",
        "      first_neuron = params['first_filter']\n",
        "    else:\n",
        "      first_neuron = params['first_neuron']\n",
        "\n",
        "    out = []\n",
        "    n = first_neuron\n",
        "\n",
        "    # the case where hidden_layers is zero\n",
        "    if layers == 0:\n",
        "        return [0]\n",
        "\n",
        "    # the cases where an angle is applied\n",
        "    if isinstance(shape, float):\n",
        "\n",
        "        for i in range(layers):\n",
        "\n",
        "            n *= 1 - shape\n",
        "\n",
        "            if n > last_neuron:\n",
        "                out.append(int(n))\n",
        "            else:\n",
        "                out.append(last_neuron)\n",
        "\n",
        "    # the case where a rectantular shape is used\n",
        "    elif shape == 'brick':\n",
        "        out = [first_neuron] * layers\n",
        "\n",
        "    elif shape == 'funnel':\n",
        "        for i in range(layers + 1):\n",
        "            n -= int((first_neuron - last_neuron) / layers)\n",
        "            out.append(n)\n",
        "        out.pop(-1)\n",
        "\n",
        "    elif shape == 'triangle':\n",
        "        out = np.linspace(first_neuron,\n",
        "                          last_neuron,\n",
        "                          layers+2,\n",
        "                          dtype=int).tolist()\n",
        "\n",
        "        out.pop(0)\n",
        "        out.pop(-1)\n",
        "        out.reverse()\n",
        "\n",
        "    else:\n",
        "        message = \"'shapes' must be float or in ['funnel', 'brick', 'triangle']\"\n",
        "        raise TalosParamsError(message)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def hidden_layers_customized(model, params, last_neuron, network_type):\n",
        "    '''HIDDEN LAYER Generator\n",
        "\n",
        "    NOTE: 'shapes', 'first_neuron', 'dropout', and 'hidden_layers' need\n",
        "    to be present in the params dictionary.\n",
        "\n",
        "    Hidden layer generation for the cases where number\n",
        "    of layers is used as a variable in the optimization process.\n",
        "    Handles things in a way where any number of layers can be tried\n",
        "    with matching hyperparameters.'''\n",
        "\n",
        "    # check for the params that are required for hidden_layers\n",
        "\n",
        "    from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, SimpleRNN, GRU, LSTM\n",
        "    # from .network_shape import network_shape\n",
        "    from talos.utils.exceptions import TalosParamsError\n",
        "\n",
        "    if network_type == 0:\n",
        "      required = ['shapes', 'first_neuron', 'dropout', 'hidden_layers', 'rnn_units']\n",
        "    elif network_type == 1:\n",
        "      required = ['shapes', 'first_neuron', 'dropout', 'hidden_layers', 'gru_units']\n",
        "    elif network_type == 2:\n",
        "      required = ['shapes', 'first_neuron', 'dropout', 'hidden_layers', 'lstm_units']\n",
        "    elif network_type == 3:\n",
        "      required = ['shapes', 'first_filter', 'dropout', 'hidden_layers', 'kernel_size', 'activation', 'filter']\n",
        "\n",
        "\n",
        "\n",
        "    for param in required:\n",
        "        if param not in params:\n",
        "            message = \"hidden_layers requires '\" + param + \"' in params\"\n",
        "            raise TalosParamsError(message)\n",
        "\n",
        "    layer_neurons = network_shape_customized(params, last_neuron, network_type)\n",
        "    # network_type == 0 --> SimpleRNN\n",
        "    # network_type == 1 --> GRU\n",
        "    # network_type == 2 --> LSTM\n",
        "    # network_type == 3 --> CONV1D\n",
        "\n",
        "    if network_type == 0:\n",
        "        for i in range(params['hidden_layers']):\n",
        "            if params['hidden_layers'] == 0:\n",
        "                model.add(SimpleRNN(layer_neurons[i], return_sequences=False,))\n",
        "            else:\n",
        "                if i == params['hidden_layers'] - 1:\n",
        "                    model.add(SimpleRNN(layer_neurons[i], return_sequences=False,))\n",
        "                else:\n",
        "                    model.add(SimpleRNN(layer_neurons[i], return_sequences=True,))\n",
        "    elif network_type == 1:\n",
        "         for i in range(params['hidden_layers']):\n",
        "          model.add(GRU(layer_neurons[i], return_sequences=True))\n",
        "    elif network_type == 2:\n",
        "         for i in range(params['hidden_layers']):\n",
        "          model.add(LSTM(layer_neurons[i], return_sequences=True))\n",
        "    elif network_type == 3:\n",
        "      for i in range(params['hidden_layers']):\n",
        "\n",
        "          model.add(Conv1D(layer_neurons[i],\n",
        "                          kernel_size=params.get('kernel_size'),\n",
        "                           padding = 'same', activation='relu'))\n",
        "          model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
        "          model.add(Dropout(params['dropout']))\n",
        "    else:\n",
        "     message = \"Model not supported\"\n",
        "     raise TalosParamsError(message)\n",
        "\n"
      ],
      "metadata": {
        "id": "DwSkU0oqhJwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topology shapes**\n",
        "\n",
        "Topology shapes are package-specific names where ‘brick’ assigns the\n",
        "same number of neurons in each layer, ‘triangle’ decreases the number of\n",
        "neurons by a constant number with each layer so that the shape resembles a\n",
        "triangle, and ‘funnel’ decreases the number of neurons by the floor of the\n",
        "difference between the specified number of neurons in the first layer and last\n",
        "layer divided by the number of desired hidden layers, resulting in a funnel\n",
        "shape."
      ],
      "metadata": {
        "id": "J2Xu3ypFvUtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 Simple Recurrent Neural Network (RNN)"
      ],
      "metadata": {
        "id": "kzoIsvJsvBbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SimpleRNN_fn(x_train, y_train, x_val, y_val, params):\n",
        "\t  # Step 1: reset the tensorflow backend session.\n",
        "    tf.keras.backend.clear_session()\n",
        "    # Step 2: Define the model with variable hyperparameters.\n",
        "    dropout = float(params['dropout'])\n",
        "    lr = float(params['lr'])\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.SimpleRNN(params['rnn_units'], return_sequences=True,\n",
        "                     input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    hidden_layers_customized(model, params, 1, 0)\n",
        "    model.add(tf.keras.layers.Dropout(dropout))\n",
        "    model.add(tf.keras.layers.Dense(1, activation='linear',))\n",
        "    \n",
        "    model.compile(\n",
        "        loss='mse',\n",
        "        optimizer = params['optimiz'](learning_rate=lr),\n",
        "        # metrics=['mse']\n",
        "    )\n",
        "\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                  patience=5)\n",
        "    \n",
        "    history = model.fit(\n",
        "        x_train, y_train, \n",
        "        epochs=params['epochs'], \n",
        "        batch_size=params['batch_size'], \n",
        "        verbose=0,\n",
        "        validation_data=[x_val, y_val],\n",
        "        callbacks=[stop_early], \n",
        "    )\n",
        "\n",
        "    return history, model, \n",
        "\n",
        "from tensorflow.keras.optimizers import Adam, Nadam\n",
        "\n",
        "para = {\n",
        "    'batch_size': [100],  # 100, 500, 1250,\n",
        "    'epochs': [2],  # 100, 200\n",
        "    'shapes': ['brick'],  # <<< required\n",
        "    'first_neuron': [64],  # 64,                   # <<< required\n",
        "    'hidden_layers': [4],  # 2, 3                  # <<< required\n",
        "    'dropout': [0, ],  # 0.25                      # <<< required\n",
        "    'rnn_units': [128],  # 128                     # <<< required\n",
        "    'optimiz': [Nadam, Adam],\n",
        "    'lr': [0.001]\n",
        "}\n",
        "\n",
        "scan_results_SimpleRNN = ta.Scan(x=X_train,\n",
        "                 y=y_train,\n",
        "                 params=para,\n",
        "                 model=SimpleRNN_fn,\n",
        "                 experiment_name='HyperparameterTuning_SimpleRNN',\n",
        "                 x_val=X_valid,\n",
        "                 y_val=y_valid,\n",
        "                 disable_progress_bar=False, print_params=True)"
      ],
      "metadata": {
        "id": "M93LBfGTuvgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Selection"
      ],
      "metadata": {
        "id": "ff2aCAAh-4DV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scan_results_SimpleRNN.details"
      ],
      "metadata": {
        "id": "Byv0cme6DEQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = ta.Reporting('/content/HyperparameterTuning_SimpleRNN/020722111217.csv')\n",
        "\n",
        "# returns the results dataframe\n",
        "r.data.sort_values(by=['val_loss'], ascending=True)\n",
        "#r.plot_kde\n",
        "\n",
        "# returns the highest value for 'val_fmeasure'\n",
        "#r.high('mse')\n",
        "\n",
        "# returns the number of rounds it took to find best model\n",
        "#r.rounds2high()\n",
        "\n",
        "# draws a histogram for 'val_acc'\n",
        "#r.plot_hist()\n"
      ],
      "metadata": {
        "id": "is68QDDBmKyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get correlation for hyperparameters against a metric\n",
        "r.correlate('val_loss', ['epochs', 'loss', 'val_loss'])"
      ],
      "metadata": {
        "id": "up2TmQknXtyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.plot_corr('val_loss', ['loss', 'val_loss', 'epochs', 'rnn_units', 'dropout'])"
      ],
      "metadata": {
        "id": "CDNJLD9aZY5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best model index with highest 'val_categorical_accuracy' \n",
        "model_id = scan_results_SimpleRNN.data['val_loss'].astype('float').argmin() - 0\n",
        "\n",
        "model_id"
      ],
      "metadata": {
        "id": "yA_uVZm1OWsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear any previous TensorFlow session.\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Load the model parameters from the scanner.\n",
        "from tensorflow.keras.models import model_from_json\n",
        "model = model_from_json(scan_results_SimpleRNN.saved_models[model_id])\n",
        "model.set_weights(scan_results_SimpleRNN.saved_weights[model_id])\n",
        "model.summary()\n",
        "# model.save('./best_model')\n",
        "\n",
        "# Load that model\n",
        "# my_tf_saved_model = tf.keras.models.load_model('./saved_models/my_tf_model')\n",
        "# my_tf_saved_model.summary()"
      ],
      "metadata": {
        "id": "8tm5QhzROrMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='model.png', show_shapes=True)"
      ],
      "metadata": {
        "id": "gdoZ_A18HjEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_history = scan_results_SimpleRNN.round_history[model_id]\n",
        "\n",
        "# LOSS CURVE\n",
        "# Plot train loss and validation loss\n",
        "def plot_loss(history):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['loss'])\n",
        "    plt.plot(history['val_loss'])\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['Train loss', 'Validation loss'], loc='upper right')\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "plot_loss(model_history)"
      ],
      "metadata": {
        "id": "RnQqQcHtU5hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Prediction"
      ],
      "metadata": {
        "id": "QDtfSlL8liFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make prediction\n",
        "def prediction(model):\n",
        "    prediction = model.predict(X_test)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "prediction_simpleRNN = prediction(model)\n",
        "\n",
        "# prediction_simpleRNN.shape"
      ],
      "metadata": {
        "id": "svDUmG1PMvRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles = [\n",
        "    \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\",\n",
        "]\n",
        "\n",
        "\n",
        "s = pd.DataFrame(data=prediction_simpleRNN, index=None, columns=[f'{x}' for x in titles], dtype=None, copy=None)\n",
        "print(s)\n",
        "y = pd.DataFrame(data=y_test, index=None, columns=[f'{x}' for x in titles], dtype=None, copy=None)\n",
        "\n",
        "a = 2  # number of rows\n",
        "b = 3  # number of columns\n",
        "c = 1  # initialize plot counter\n",
        "fig = plt.figure(figsize=(12, 9))\n",
        "for i in titles:\n",
        "    plt.subplot(a, b, c)\n",
        "    plt.title(f'{i}')\n",
        "    axs = s[f'{i}'].plot(label=f'{i} pred')\n",
        "    y[f'{i}'].plot(ax=axs, label=f'{i} test')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xlabel('Time (day)')\n",
        "    plt.ylabel('Daily water consumption ($m^3$/capita.day)')\n",
        "    c = c + 1\n",
        "plt.subplots_adjust(.5)\n",
        "fig.suptitle('Main title', fontsize=18, y=1)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i_zf8VvUOsKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to calculate MAE and RMSE\n",
        "def evaluate_prediction(predictions, actual, model_name):\n",
        "    for i, j in zip(range(0, predictions.shape[1]), titles):\n",
        "        rsme = np.sqrt((mean_squared_error(predictions[i], actual[i])))\n",
        "        mae = mean_absolute_error(predictions[i], actual[i])\n",
        "        r2 = r2_score(predictions[i], actual[i])\n",
        "        # max_err = max_error(actual, predictions)\n",
        "        print(model_name + ' performance of', j, 'feature:')\n",
        "        print('R^2 of: {:.4f}'.format(r2))\n",
        "        print('Mean Absolute Error of: {:.4f}'.format(mae))\n",
        "        print('Root Mean Square Error of: {:.4f}'.format(rsme))\n",
        "        # print('Max_error: {:.4f}'.format(max_err))\n",
        "        print('')\n",
        "    rsme = np.sqrt((mean_squared_error(predictions, actual)))\n",
        "    mae = mean_absolute_error(actual, predictions)\n",
        "    r2 = r2_score(actual, predictions)\n",
        "    # max_err = max_error(actual, predictions)\n",
        "    print(model_name + ' average performance:')\n",
        "    print('R^2: {:.4f}'.format(r2))\n",
        "    print('Mean Absolute Error: {:.4f}'.format(mae))\n",
        "    print('Root Mean Square Error: {:.4f}'.format(rsme))\n",
        "    # print('Max_error: {:.4f}'.format(max_err))\n",
        "    print('')\n",
        "\n",
        "\n",
        "evaluate_prediction(prediction_simpleRNN, y_test, 'LSTM')"
      ],
      "metadata": {
        "id": "Y1jw5-6qO85d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Prediction Out of sample"
      ],
      "metadata": {
        "id": "5B516RUKPAMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_out_of_sample(classifier, series, n_ahead=1):\n",
        "    series = np.copy(series)\n",
        "    series = series[-1]\n",
        "    prediction = np.zeros((n_ahead, y_train.shape[1]))\n",
        "    for i in range(n_ahead):\n",
        "        current_x = series[-X_train.shape[1]:]\n",
        "        current_x = current_x[newaxis, :, :]\n",
        "        next_pred = classifier.predict(current_x)\n",
        "        prediction[i] = next_pred\n",
        "        next_pred_scaled = scaler.transform(next_pred)\n",
        "        series = np.concatenate((series, next_pred_scaled), axis=0)\n",
        "    return prediction\n"
      ],
      "metadata": {
        "id": "vDoaAJECO_08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsione test set e proiezione nel futuro di n step\n",
        "prediction = predict_out_of_sample(model, X_test, 60)\n",
        "\n",
        "print(prediction)\n",
        "print(prediction.shape)"
      ],
      "metadata": {
        "id": "e3kVuSATPJSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.tseries.offsets import DateOffset\n",
        "\n",
        "add_dates = [df.index[-1] + DateOffset(days=x) for x in range(0, 60 + 1)]\n",
        "future_dates = pd.DataFrame(index=add_dates[1:], columns=df.columns)\n",
        "df_predict = pd.DataFrame(prediction, index=future_dates[-X_test.shape[0]:].index, columns=df.columns)\n",
        "df = df.append(df_predict)"
      ],
      "metadata": {
        "id": "Ua1QBEJbPK2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in titles:\n",
        "    plt.title(f'{i}')\n",
        "    plt.plot(df[f'{i}'][-200:-59], label=f'{i} pred', color='g')\n",
        "    plt.plot(df[f'{i}'][-60:], label=f'{i} pred', color='r')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xlabel('Time (day)')\n",
        "    plt.ylabel('Daily water consumption ($m^3$/capita.day)')\n",
        "    fig.suptitle('Main title', fontsize=18, y=1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Y6xS-VSUPMU0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}