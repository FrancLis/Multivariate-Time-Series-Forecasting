{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di Multivariate_Timeseries_Forecasting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPsl1cumd5JDefx8G0qJtQD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancLis/Multivariate-Time-Series-Forecasting/blob/main/Copia_di_Multivariate_Timeefssssseries_Forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fast_ml\n",
        "!pip install talos\n",
        "!pip install kats\n",
        "!pip install scipy"
      ],
      "metadata": {
        "id": "j3_7phLIJtws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value = 0\n",
        "\n",
        "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as python_random\n",
        "\n",
        "# The below is necessary for starting Numpy generated random numbers\n",
        "# in a well-defined initial state.\n",
        "np.random.seed(123)\n",
        "\n",
        "# The below is necessary for starting core Python generated random numbers\n",
        "# in a well-defined state.\n",
        "python_random.seed(123)\n",
        "\n",
        "# The below set_seed() will make random number generation\n",
        "# in the TensorFlow backend have a well-defined initial state.\n",
        "# For further details, see:\n",
        "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
        "tf.random.set_seed(1234)\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import talos as ta\n",
        "from matplotlib import pyplot as plt\n",
        "from fast_ml.model_development import train_valid_test_split\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import MinMaxScaler, PowerTransformer, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, max_error, mean_absolute_error\n",
        "from tensorflow.keras import Sequential, layers, callbacks\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, GRU, Bidirectional, SimpleRNN, Conv1D, MaxPooling1D, Flatten\n"
      ],
      "metadata": {
        "id": "Xhf0TVeCAITf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Acquisition"
      ],
      "metadata": {
        "id": "PED-JLJ-TiV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Csv\n",
        "file = r\"/content/PG.csv\"\n",
        "df = pd.read_csv(file, parse_dates=['Date'], index_col='Date')\n",
        "plt.style.use('seaborn')"
      ],
      "metadata": {
        "id": "EGg2pcS8SQrK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Visualization..."
      ],
      "metadata": {
        "id": "ONXQADopI4Vc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data prepocessing"
      ],
      "metadata": {
        "id": "z8LPDz-6sU1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 Data Splitting"
      ],
      "metadata": {
        "id": "qfHlFGwFV5dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fast_ml library\n",
        "X_train, y_train, X_valid, y_valid, X_test, y_test = train_valid_test_split(df, target='Close', method='sorted',\n",
        "                                                                            sort_by_col='Date', train_size=0.6,\n",
        "                                                                            valid_size=0.2, test_size=0.2)\n",
        "print('X_train.shape:', X_train.shape, 'y_train.shape:', y_train.shape)\n",
        "print('X_valid.shape:', X_valid.shape, 'y_valid.shape:', y_valid.shape)\n",
        "print('X_test.shape:', X_test.shape, 'y_test.shape:', y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77I7sM2MV9pj",
        "outputId": "bdc1b52d-0e35-428d-dfff-b9297fa1987e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train.shape: (7887, 5) y_train.shape: (7887,)\n",
            "X_valid.shape: (2629, 5) y_valid.shape: (2629,)\n",
            "X_test.shape: (2629, 5) y_test.shape: (2629,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 Data Transformation"
      ],
      "metadata": {
        "id": "O1lRwfEWV-hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "'''\n",
        "# Other transformers\n",
        "# StandardScaler\n",
        "st_scaler = StandardScaler()\n",
        "X_train = st_scaler.fit_transform(X_train)\n",
        "X_test = st_scaler.transform(X_test)\n",
        "\n",
        "# PowerTransformer\n",
        "pt = PowerTransformer()\n",
        "X_train = pt.fit_transform(X_train)\n",
        "X_test = pt.transform(X_test)\n",
        "'''\n"
      ],
      "metadata": {
        "id": "qO6TW_wGWEFy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "00e5a5de-a0d6-41cd-95fe-d5095225cae9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Other transformers\\n# StandardScaler\\nst_scaler = StandardScaler()\\nX_train = st_scaler.fit_transform(X_train)\\nX_test = st_scaler.transform(X_test)\\n\\n# PowerTransformer\\npt = PowerTransformer()\\nX_train = pt.fit_transform(X_train)\\nX_test = pt.transform(X_test)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4 Set \"***Window size***\""
      ],
      "metadata": {
        "id": "5iIDNdRfWD0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 3D input\n",
        "def create_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        v = X[i:i + time_steps, :]\n",
        "        Xs.append(v)\n",
        "        ys.append(y[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "\n",
        "TIME_STEPS = 10\n",
        "X_test, y_test = create_dataset(X_test, y_test, TIME_STEPS)\n",
        "X_train, y_train = create_dataset(X_train, y_train, TIME_STEPS)\n",
        "X_valid, y_valid = create_dataset(X_valid, y_valid, TIME_STEPS)\n",
        "\n",
        "print('All shapes are: (batch, time, features)')\n",
        "print('X_train.shape:', X_train.shape, 'y_train.shape:', y_train.shape)\n",
        "print('X_valid.shape:', X_valid.shape, 'y_valid.shape:', y_valid.shape)\n",
        "print('X_test.shape:', X_test.shape, 'y_test.shape:', y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2BSrAGeWfA8",
        "outputId": "23b29cc4-14d2-431b-9b83-0c7a18783859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All shapes are: (batch, time, features)\n",
            "X_train.shape: (1490, 10, 5) y_train.shape: (1490,)\n",
            "X_valid.shape: (490, 10, 5) y_valid.shape: (490,)\n",
            "X_test.shape: (490, 10, 5) y_test.shape: (490,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Choice and Learning"
      ],
      "metadata": {
        "id": "1C5D4-G1XC32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from talos.utils import hidden_layers"
      ],
      "metadata": {
        "id": "m6k_ZsXzICT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def network_shape(params, last_neuron, network_type):\n",
        "\n",
        "    '''Provides the ability to include network shape in experiments. If params\n",
        "    dictionary for the round contains float value for params['shapes'] then\n",
        "    a linear contraction towards the last_neuron value. The higher the value,\n",
        "    the fewer layers it takes to reach lesser than last_neuron.\n",
        "    Supports three inbuilt shapes 'brick', 'funnel', and 'triangle'.\n",
        "    params : dict\n",
        "         Scan() params for a single roundself.\n",
        "    last_neuron : int\n",
        "         Number of neurons on the output layer in the Keras model.\n",
        "    '''\n",
        "    import numpy as np\n",
        "    from talos.utils.exceptions import TalosParamsError\n",
        "\n",
        "    layers = params['hidden_layers']\n",
        "    shape = params['shapes']\n",
        "    # network_type == 0 --> SimpleRNN\n",
        "    # network_type == 1 --> GRU\n",
        "    # network_type == 2 --> LSTM\n",
        "    # network_type == 3 --> CONV1D\n",
        "    if network_type == 3:\n",
        "      first_neuron = params['first_filter']\n",
        "    else:\n",
        "      first_neuron = params['first_neuron']\n",
        "\n",
        "    out = []\n",
        "    n = first_neuron\n",
        "\n",
        "    # the case where hidden_layers is zero\n",
        "    if layers == 0:\n",
        "        return [0]\n",
        "\n",
        "    # the cases where an angle is applied\n",
        "    if isinstance(shape, float):\n",
        "\n",
        "        for i in range(layers):\n",
        "\n",
        "            n *= 1 - shape\n",
        "\n",
        "            if n > last_neuron:\n",
        "                out.append(int(n))\n",
        "            else:\n",
        "                out.append(last_neuron)\n",
        "\n",
        "    # the case where a rectantular shape is used\n",
        "    elif shape == 'brick':\n",
        "        out = [first_neuron] * layers\n",
        "\n",
        "    elif shape == 'funnel':\n",
        "        for i in range(layers + 1):\n",
        "            n -= int((first_neuron - last_neuron) / layers)\n",
        "            out.append(n)\n",
        "        out.pop(-1)\n",
        "\n",
        "    elif shape == 'triangle':\n",
        "        out = np.linspace(first_neuron,\n",
        "                          last_neuron,\n",
        "                          layers+2,\n",
        "                          dtype=int).tolist()\n",
        "\n",
        "        out.pop(0)\n",
        "        out.pop(-1)\n",
        "        out.reverse()\n",
        "\n",
        "    else:\n",
        "        message = \"'shapes' must be float or in ['funnel', 'brick', 'triangle']\"\n",
        "        raise TalosParamsError(message)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def hidden_layers(model, params, last_neuron, network_type):\n",
        "    '''HIDDEN LAYER Generator\n",
        "\n",
        "    NOTE: 'shapes', 'first_neuron', 'dropout', and 'hidden_layers' need\n",
        "    to be present in the params dictionary.\n",
        "\n",
        "    Hidden layer generation for the cases where number\n",
        "    of layers is used as a variable in the optimization process.\n",
        "    Handles things in a way where any number of layers can be tried\n",
        "    with matching hyperparameters.'''\n",
        "\n",
        "    # check for the params that are required for hidden_layers\n",
        "\n",
        "    from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, SimpleRNN, GRU, LSTM\n",
        "    # from .network_shape import network_shape\n",
        "    from talos.utils.exceptions import TalosParamsError\n",
        "\n",
        "    if network_type == 0:\n",
        "      required = ['shapes', 'first_neuron', 'dropout', 'hidden_layers', 'rnn_units']\n",
        "    elif network_type == 1:\n",
        "      required = ['shapes', 'first_neuron', 'dropout', 'hidden_layers', 'gru_units']\n",
        "    elif network_type == 2:\n",
        "      required = ['shapes', 'first_neuron', 'dropout', 'hidden_layers', 'lstm_units']\n",
        "    elif network_type == 3:\n",
        "      required = ['shapes', 'first_filter', 'dropout', 'hidden_layers', 'kernel_size', 'activation', 'filter']\n",
        "\n",
        "\n",
        "\n",
        "    for param in required:\n",
        "        if param not in params:\n",
        "            message = \"hidden_layers requires '\" + param + \"' in params\"\n",
        "            raise TalosParamsError(message)\n",
        "\n",
        "    layer_neurons = network_shape(params, last_neuron, network_type)\n",
        "    # network_type == 0 --> SimpleRNN\n",
        "    # network_type == 1 --> GRU\n",
        "    # network_type == 2 --> LSTM\n",
        "    # network_type == 3 --> CONV1D\n",
        "\n",
        "    if network_type == 0:\n",
        "        for i in range(params['hidden_layers']):\n",
        "            if params['hidden_layers'] == 0:\n",
        "                model.add(SimpleRNN(layer_neurons[i], return_sequences=False,))\n",
        "            else:\n",
        "                if i == params['hidden_layers'] - 1:\n",
        "                    model.add(SimpleRNN(layer_neurons[i], return_sequences=False,))\n",
        "                else:\n",
        "                    model.add(SimpleRNN(layer_neurons[i], return_sequences=True,))\n",
        "    elif network_type == 1:\n",
        "         for i in range(params['hidden_layers']):\n",
        "          model.add(GRU(layer_neurons[i], return_sequences=True))\n",
        "    elif network_type == 2:\n",
        "         for i in range(params['hidden_layers']):\n",
        "          model.add(LSTM(layer_neurons[i], return_sequences=True))\n",
        "    elif network_type == 3:\n",
        "      for i in range(params['hidden_layers']):\n",
        "\n",
        "          model.add(Conv1D(layer_neurons[i],\n",
        "                          kernel_size=params.get('kernel_size'),\n",
        "                           padding = 'same', activation='relu'))\n",
        "          model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
        "          model.add(Dropout(params['dropout']))\n",
        "    else:\n",
        "     message = \"Model not supported\"\n",
        "     raise TalosParamsError(message)\n",
        "\n"
      ],
      "metadata": {
        "id": "DwSkU0oqhJwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topology shapes are package-specific names where ‘brick’ assigns the\n",
        "same number of neurons in each layer, ‘triangle’ decreases the number of\n",
        "neurons by a constant number with each layer so that the shape resembles a\n",
        "triangle, and ‘funnel’ decreases the number of neurons by the floor of the\n",
        "difference between the specified number of neurons in the first layer and last\n",
        "layer divided by the number of desired hidden layers, resulting in a funnel\n",
        "shape."
      ],
      "metadata": {
        "id": "J2Xu3ypFvUtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 Simple Recurrent Neural Network (RNN)"
      ],
      "metadata": {
        "id": "kzoIsvJsvBbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SimpleRNN_fn(x_train, y_train, x_val, y_val, params):\n",
        "\t  # Step 1: reset the tensorflow backend session.\n",
        "    tf.keras.backend.clear_session()\n",
        "    # Step 2: Define the model with variable hyperparameters.\n",
        "    dropout = float(params['dropout'])\n",
        "    lr = float(params['lr'])\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.SimpleRNN(params['rnn_units'], return_sequences=True,\n",
        "                     input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    hidden_layers(model, params, 1, 0)\n",
        "    model.add(tf.keras.layers.Dropout(dropout))\n",
        "    model.add(tf.keras.layers.Dense(1, activation='linear',))\n",
        "    \n",
        "    model.compile(\n",
        "        loss='mse',\n",
        "        optimizer = params['optimiz'](learning_rate=lr),\n",
        "        # metrics=['mse']\n",
        "    )\n",
        "\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                  patience=5)\n",
        "    \n",
        "    history = model.fit(\n",
        "        x_train, y_train, \n",
        "        epochs=params['epochs'], \n",
        "        batch_size=params['batch_size'], \n",
        "        verbose=0,\n",
        "        validation_data=[x_val, y_val],\n",
        "        callbacks=[stop_early], \n",
        "    )\n",
        "\n",
        "    return history, model, \n",
        "\n",
        "from tensorflow.keras.optimizers import Adam, Nadam\n",
        "para = {\n",
        "    'batch_size': [1250],  # 100, 500, 1250,\n",
        "    'epochs': [2],  # 100, 200\n",
        "    'shapes': ['funnel', 'brick', 'triangle'],  # <<< required\n",
        "    'first_neuron': [64],  # 64,                   # <<< required\n",
        "    'hidden_layers': [2, 4],  # 2, 3                   # <<< required\n",
        "    'dropout': [0, ],  # 0.25                         # <<< required\n",
        "    'rnn_units': [128],  # 128                       # <<< required\n",
        "    'optimiz': [Nadam, Adam],\n",
        "    'lr': [0.001]\n",
        "}\n",
        "\n",
        "scan_results_SimpleRNN = ta.Scan(x=X_train,\n",
        "                 y=y_train,\n",
        "                 params=para,\n",
        "                 model=SimpleRNN_fn,\n",
        "                 experiment_name='HyperparameterTuning_SimpleRNN',\n",
        "                 x_val=X_valid,\n",
        "                 y_val=y_valid,\n",
        "                 disable_progress_bar=False, print_params=True)"
      ],
      "metadata": {
        "id": "M93LBfGTuvgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Selection"
      ],
      "metadata": {
        "id": "ff2aCAAh-4DV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scan_results_SimpleRNN.details"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byv0cme6DEQo",
        "outputId": "cfdcd312-896e-41b5-c858-aa5dfea2efc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "experiment_name        HyperparameterTuning_SimpleRNN\n",
              "random_method                        uniform_mersenne\n",
              "reduction_method                                 None\n",
              "reduction_interval                                 50\n",
              "reduction_window                                   20\n",
              "reduction_threshold                               0.2\n",
              "reduction_metric                              val_acc\n",
              "complete_time                          02/07/22/17:54\n",
              "x_shape                                 (1490, 10, 5)\n",
              "y_shape                                       (1490,)\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ta.Reporting(scan_results_SimpleRNN)"
      ],
      "metadata": {
        "id": "_JU2nC8AQS6p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "249998cd-b89a-4a6d-ea7c-5fe5df437c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<talos.commands.analyze.Analyze at 0x7febe81c13d0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = ta.Reporting('/content/HyperparameterTuning_SimpleRNN/020722111217.csv')\n",
        "\n",
        "# returns the results dataframe\n",
        "r.data.sort_values(by=['val_loss'], ascending=True)\n",
        "#r.plot_kde\n",
        "\n",
        "# returns the highest value for 'val_fmeasure'\n",
        "#r.high('mse')\n",
        "\n",
        "# returns the number of rounds it took to find best model\n",
        "#r.rounds2high()\n",
        "\n",
        "# draws a histogram for 'val_acc'\n",
        "#r.plot_hist()\n"
      ],
      "metadata": {
        "id": "is68QDDBmKyu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "31155cd9-bdf2-474c-e166-d9f03527d1eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d77a0ccfa765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReporting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/HyperparameterTuning_SimpleRNN/020722111217.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# returns the results dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#r.plot_kde\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/talos/commands/analyze.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/HyperparameterTuning_SimpleRNN/020722111217.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KsU0xp20Yj3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get correlation for hyperparameters against a metric\n",
        "r.correlate('val_loss', ['epochs', 'loss', 'val_loss'])"
      ],
      "metadata": {
        "id": "up2TmQknXtyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r.plot_corr('val_loss', ['loss', 'val_loss', 'epochs', 'rnn_units', 'dropout'])"
      ],
      "metadata": {
        "id": "CDNJLD9aZY5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best model index with highest 'val_categorical_accuracy' \n",
        "model_id = scan_results_SimpleRNN.data['val_loss'].astype('float').argmin() - 0\n",
        "\n",
        "model_id"
      ],
      "metadata": {
        "id": "yA_uVZm1OWsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear any previous TensorFlow session.\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Load the model parameters from the scanner.\n",
        "from tensorflow.keras.models import model_from_json\n",
        "model = model_from_json(scan_results_SimpleRNN.saved_models[model_id])\n",
        "model.set_weights(scan_results_SimpleRNN.saved_weights[model_id])\n",
        "model.summary()\n",
        "model.save('./best_model')\n",
        "\n",
        "# Load that model\n",
        "# my_tf_saved_model = tf.keras.models.load_model('./saved_models/my_tf_model')\n",
        "# my_tf_saved_model.summary()"
      ],
      "metadata": {
        "id": "8tm5QhzROrMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='model.png', show_shapes=True)"
      ],
      "metadata": {
        "id": "gdoZ_A18HjEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_history = scan_results_SimpleRNN.round_history[model_id]\n",
        "\n",
        "# LOSS CURVE\n",
        "# Plot train loss and validation loss\n",
        "def plot_loss(history):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['loss'])\n",
        "    plt.plot(history['val_loss'])\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['Train loss', 'Validation loss'], loc='upper right')\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "plot_loss(model_history)"
      ],
      "metadata": {
        "id": "RnQqQcHtU5hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Prediction"
      ],
      "metadata": {
        "id": "QDtfSlL8liFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make prediction\n",
        "def prediction(model):\n",
        "    prediction = model.predict(X_test)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "prediction_simpleRNN = prediction(model)\n",
        "\n",
        "# prediction_simpleRNN.shape\n",
        "\n",
        "# Plot true future vs prediction\n",
        "def plot_future(prediction, y_test):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    range_future = len(prediction)\n",
        "    plt.plot(np.arange(range_future), np.array(y_test),\n",
        "             label='True Future')\n",
        "    plt.plot(np.arange(range_future), np.array(prediction),\n",
        "             label='Prediction')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xlabel('Time (day)')\n",
        "    plt.ylabel('Daily water consumption ($m^3$/capita.day)')\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "plot_future(prediction_simpleRNN, y_test)"
      ],
      "metadata": {
        "id": "svDUmG1PMvRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to calculate MAE and RMSE\n",
        "def evaluate_prediction(predictions, actual, model_name):\n",
        "    rsme = np.sqrt((mean_squared_error(predictions, actual)))\n",
        "    mae = mean_absolute_error(actual, predictions)\n",
        "    r2 = r2_score(actual, predictions)\n",
        "    max_err = max_error(actual, predictions)\n",
        "\n",
        "    print(model_name + ':')\n",
        "    print('R^2: {:.4f}'.format(r2))\n",
        "    print('Mean Absolute Error: {:.4f}'.format(mae))\n",
        "    print('Root Mean Square Error: {:.4f}'.format(rsme))\n",
        "    print('Max_error: {:.4f}'.format(max_err))\n",
        "    print('')\n",
        "\n",
        "\n",
        "evaluate_prediction(prediction_simpleRNN, y_test, 'LSTM')"
      ],
      "metadata": {
        "id": "ArifyG1dUPYZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}