{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Data_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancLis/Multivariate-Time-Series-Forecasting/blob/main/2_Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fast_ml\n",
        "!pip install talos\n",
        "!pip install kats\n",
        "!pip install scipy"
      ],
      "metadata": {
        "id": "j3_7phLIJtws",
        "outputId": "4bf34521-0489-4804-c6e9-dbb5130db960",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fast_ml\n",
            "  Downloading fast_ml-3.68-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▉                        | 10 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 20 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 30 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 42 kB 415 kB/s \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value = 0\n",
        "\n",
        "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as python_random\n",
        "\n",
        "# The below is necessary for starting Numpy generated random numbers\n",
        "# in a well-defined initial state.\n",
        "np.random.seed(123)\n",
        "\n",
        "# The below is necessary for starting core Python generated random numbers\n",
        "# in a well-defined state.\n",
        "python_random.seed(123)\n",
        "\n",
        "# The below set_seed() will make random number generation\n",
        "# in the TensorFlow backend have a well-defined initial state.\n",
        "# For further details, see:\n",
        "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
        "tf.random.set_seed(1234)\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import talos as ta\n",
        "from matplotlib import pyplot as plt\n",
        "from fast_ml.model_development import train_valid_test_split\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import MinMaxScaler, PowerTransformer, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, max_error, mean_absolute_error\n",
        "from tensorflow.keras import Sequential, layers, callbacks\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, GRU, Bidirectional, SimpleRNN, Conv1D, MaxPooling1D, Flatten\n"
      ],
      "metadata": {
        "id": "Xhf0TVeCAITf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Acquisition"
      ],
      "metadata": {
        "id": "PED-JLJ-TiV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Csv\n",
        "file = r\"/content/PG.csv\"\n",
        "df = pd.read_csv(file, parse_dates=['Date'], index_col='Date')\n",
        "plt.style.use('seaborn')"
      ],
      "metadata": {
        "id": "EGg2pcS8SQrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Visualization..."
      ],
      "metadata": {
        "id": "ONXQADopI4Vc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data prepocessing"
      ],
      "metadata": {
        "id": "z8LPDz-6sU1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 Data Cleaning"
      ],
      "metadata": {
        "id": "-WHi3rrBVjpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "EVu4HgVWVjbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There aren't missing value"
      ],
      "metadata": {
        "id": "k1se9nLXG2kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values by interpolation\n",
        "def replace_missing(attribute):\n",
        "    return attribute.interpolate(inplace=True)\n",
        "\n",
        "# replace_missing(df['Open'])\n",
        "# ...."
      ],
      "metadata": {
        "id": "9XBT5QFkx556"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect and remove outliers with IQR\n",
        "def detect_remove_outliers(df, column):\n",
        "    # IQR\n",
        "    Q1 = np.percentile(df[f'{column}'], 25, interpolation='midpoint')\n",
        "    Q3 = np.percentile(df[f'{column}'], 75, interpolation='midpoint')\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Above Upper bound\n",
        "    upper = df[f'{column}'] >= (Q3 + 1.5 * IQR)\n",
        "    # print(\"Upper bound:\", upper)\n",
        "    print(\"Upper bound outliers:\", f'{column}', np.where(upper))\n",
        "\n",
        "    # Below Lower bound\n",
        "    lower = df[f'{column}'] <= (Q1 - 1.5 * IQR)\n",
        "    # print(\"Lower bound:\", lower)\n",
        "    print(\"Lower bound:\", f'{column}', np.where(lower))\n",
        "\n",
        "    # Removing the Outliers \n",
        "    # df.drop(upper, inplace = True)\n",
        "    # df.drop(lower, inplace = True)\n",
        "    \n",
        "    # print(\"New Shape: \", df.shape)\n",
        "    return\n",
        "\n",
        "# There may be potential outliers in the Volume column, but they won't be considered outliers because\n",
        "# a large volume of transactions is related to a change in the closing price\n",
        "# For the other columns it was previously verified graphically with the boxplot that there aren't outliers.\n",
        "# Also mathematically, with the IQR method, the same result is gotten.\n",
        "\n",
        "titles = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]\n",
        "\n",
        "for i in titles:\n",
        "       detect_remove_outliers(df, f'{i}')"
      ],
      "metadata": {
        "id": "If_G5IgIVvq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WS2KdjhrQTrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   It seems that these are not outliers. They are a part of the trends of the timeseries"
      ],
      "metadata": {
        "id": "FJ-5qNtOMaLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3gxFWPcCQUzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Remove** **Volume** **feature** \n",
        "\n",
        "\n",
        "It has been decided to remove the \"Volume\" column from the dataframe. In particular, it was observed that this feature is not very related to the target. Furthermore, the purpose of the algorithm is to predict the trend of the actions in the future (out of sample) and when this is done, given that the analysis is multivariate, the data of the other features will also be provided. Trading volume is a measure of how much a given financial asset has traded in a period of time. For stocks, volume is measured in the number of shares traded.\n",
        "Volume measures the number of shares traded in a stock or contracts traded in futures or options.\n",
        "\n",
        "Volume can indicate market strength, as rising markets on increasing \n",
        "\n",
        "*   Volume measures the number of shares traded in a stock or contracts traded in futures or options.\n",
        "*   Volume can indicate market strength, as rising markets on increasing volume are typically viewed as strong and healthy.\n",
        "*   When prices fall on increasing volume, the trend is gathering strength to the downside.\n",
        "\n",
        "It is therefore more difficult to predict and could damage the model."
      ],
      "metadata": {
        "id": "N7jzBFs7N1Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['Volume'], axis=1)"
      ],
      "metadata": {
        "id": "H-KWxwbJEOl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 Data Splitting"
      ],
      "metadata": {
        "id": "qfHlFGwFV5dn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "splitting given that the goal of the algorithm is to predict the trend of the share price in the future and\n",
        "therefore given that we are in a multivariate analysis we are going to predict not only the closing price but also\n",
        "the other features in the future as a consequence of the splitting have been considered both in the training phase\n",
        "nd in the test phase all the features except the volume this to train the algorithm to predict also\n",
        "the values and the features in addition to that of the close price was also considered as training since it\n",
        "can be useful to use the previous day's value to predict the next one As happens in a multivariate analysis"
      ],
      "metadata": {
        "id": "mgJCIzP0EUo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's say we want to split the data in 80:10:10 for train:valid:test dataset it was decided to use a manual\n",
        "\n",
        "train_size = 0.8\n",
        "valid_size = 0.1\n",
        "\n",
        "train_index = int(len(df) * train_size)\n",
        "\n",
        "# First we need to sort the dataset by the desired column\n",
        "df.sort_values(by='Date', ascending=True, inplace=True)\n",
        "\n",
        "df_train = df[0:train_index]\n",
        "df_rem = df[train_index:]\n",
        "\n",
        "valid_index = int(len(df) * valid_size)\n",
        "\n",
        "df_valid = df[train_index:train_index + valid_index]\n",
        "df_test = df[train_index + valid_index:]\n",
        "test_index = df_test.shape[0]\n",
        "\n",
        "X_train, y_train = df_train[['Open', 'High', 'Low', 'Close', 'Adj Close']], \\\n",
        "                   df_train[['Open', 'High', 'Low', 'Close', 'Adj Close']]\n",
        "\n",
        "X_valid, y_valid = df_valid[['Open', 'High', 'Low', 'Close', 'Adj Close']], \\\n",
        "                   df_valid[['Open', 'High', 'Low', 'Close', 'Adj Close']]\n",
        "\n",
        "X_test, y_test = df_test[['Open', 'High', 'Low', 'Close', 'Adj Close']], \\\n",
        "                 df_test[['Open', 'High', 'Low', 'Close', 'Adj Close']]\n",
        "\n",
        "print('X_train.shape:', X_train.shape, 'y_train.shape:', y_train.shape)\n",
        "print('X_valid.shape:', X_valid.shape, 'y_valid.shape:', y_valid.shape)\n",
        "print('X_test.shape:', X_test.shape, 'y_test.shape:', y_test.shape)"
      ],
      "metadata": {
        "id": "JUn_n02bEr_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 Data Transformation"
      ],
      "metadata": {
        "id": "O1lRwfEWV-hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "'''\n",
        "# Other transformers\n",
        "# StandardScaler\n",
        "st_scaler = StandardScaler()\n",
        "X_train = st_scaler.fit_transform(X_train)\n",
        "X_test = st_scaler.transform(X_test)\n",
        "\n",
        "# PowerTransformer\n",
        "pt = PowerTransformer()\n",
        "X_train = pt.fit_transform(X_train)\n",
        "X_test = pt.transform(X_test)\n",
        "'''\n",
        "# Convert y sets to numpy array\n",
        "y_train = y_train.to_numpy()\n",
        "y_valid = y_valid.to_numpy()\n",
        "y_test = y_test.to_numpy()\n"
      ],
      "metadata": {
        "id": "qO6TW_wGWEFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4 Set \"***Window size***\""
      ],
      "metadata": {
        "id": "5iIDNdRfWD0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 3D input\n",
        "def create_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        v = X[i:i + time_steps, :]\n",
        "        Xs.append(v)\n",
        "        ys.append(y[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "\n",
        "TIME_STEPS = 20\n",
        "X_train, y_train = create_dataset(X_train, y_train, TIME_STEPS)\n",
        "X_test, y_test = create_dataset(X_test, y_test, TIME_STEPS)\n",
        "X_valid, y_valid = create_dataset(X_valid, y_valid, TIME_STEPS)\n",
        "\n",
        "print('All shapes are: (batch, time, features)')\n",
        "print('X_train.shape:', X_train.shape, 'y_train.shape:', y_train.shape)\n",
        "print('X_valid.shape:', X_valid.shape, 'y_valid.shape:', y_valid.shape)\n",
        "print('X_test.shape:', X_test.shape, 'y_test.shape:', y_test.shape)"
      ],
      "metadata": {
        "id": "z2BSrAGeWfA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2.4.1 Save preprocessed dataset "
      ],
      "metadata": {
        "id": "L4e7on_dE55W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Preprocessed_data.npy', 'wb') as f:\n",
        "    np.save(f, X_train )\n",
        "    np.save(f, y_train )\n",
        "    np.save(f, X_test )\n",
        "    np.save(f, y_test)\n",
        "    np.save(f, X_valid )\n",
        "    np.save(f, y_valid )"
      ],
      "metadata": {
        "id": "DLOVDSOlE42v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you can find the numpy file of the preprocessed data in the folder Data of this repository. \n",
        "\n",
        "Here is the link:\n",
        "https://github.com/FrancLis/Multivariate-Time-Series-Forecasting/tree/main/Data"
      ],
      "metadata": {
        "id": "SXG_LQUSIt9D"
      }
    }
  ]
}